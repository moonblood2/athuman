df = pd.read_csv(r'C:\Users\A0392\PycharmProjects\ICE\abalone.data',names=['Sex','Length','Diameter','Height','Whole Weight','Shucked Weight','Viscera Weight','Shell Weight','Rings'])
df.fillna(0, inplace=True)
df.dropna(axis=0,how='all')
# print(df.describe())
# print(df.info())
corr = df.corr('kendall')
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(8, 6))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1,square=True,linewidths=.5, cbar_kws={"shrink": .5}, ax=ax);
plt.show()


# labels = df[['0']]
# datasets = df[['1', '2', '3', '4', '5', '6', '7', '8']]
#
# x_train, x_test, y_train, y_test = train_test_split(datasets, labels, test_size=0.2)
#
# # print('M', len(df[df['0'] == 'M']))
# # print('F', len(df[df['0'] == 'F']))
# # print('I', len(df[df['0'] == 'I']))
#
# names = ["Nearest Neighbors", "Linear SVM", "RBF SVM",
#          "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
#          "Naive Bayes", "QDA"]
#
# classifiers = [
#     KNeighborsClassifier(3),
#     SVC(kernel="linear", C=0.025),
#     SVC(gamma=2, C=1),
#     # GaussianProcessClassifier(1.0 * RBF(1.0)),
#     DecisionTreeClassifier(max_depth=5),
#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
#     MLPClassifier(alpha=1),
#     AdaBoostClassifier(),
#     GaussianNB(),
#     QuadraticDiscriminantAnalysis()]
#
# for n,i in enumerate(classifiers):
#     clf = i
#     clf.fit(x_train, y_train)
#     confident = clf.score(x_test, y_test)
#     print(names[n],'=',confident)
